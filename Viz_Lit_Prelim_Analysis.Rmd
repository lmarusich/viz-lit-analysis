---
title: "Visualization Literacy Analysis"
author: "Laura Marusich, Jonathan Bakdash"
date: "3/22/2022"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, knitr, ez, psychReport, superb, plotrix, emmeans, afex
               # knitr, rmarkdown, yaml, tidyverse, plyr, psych, foreign, stats, lme4,
               #              RColorBrewer, pals, car, ez, devtools, MBESS, apaTables, gridExtra, plyr,
               #              cowplot, Rmisc, afex, lsmeans, emmeans, ggsignif, effsize, TOSTER
) 

knitr::opts_chunk$set(echo = TRUE, tidy = FALSE)
options(width = 90)

dir.create(file.path(getwd(),"/plots"), showWarnings = F)
```

<!-- ## Grab raw files from Google Drive (only have to do this once) -->

<!-- ```{r cars} -->
<!-- data_folder_names <- drive_ls(path = "Viz Lit Data 2019 to Current", pattern = "Lit Data") -->

<!-- for (i in 1:4){ -->
<!--   temp_files <- drive_ls(top_files$name[i], recursive = T, type = "csv") -->
<!--   n_files <- dim(temp_files)[1] -->
<!--   for (j in 1:n_files){ -->
<!--     drive_download(temp_files[j,], overwrite = T) -->
<!--   } -->
<!-- } -->
<!-- ``` -->

## Read in data files

```{r, warning=F, message=F}
#get the first 6? characters of each data file
#get unique values of these
# this is list of subject ids

raw_file_names <- list.files("Raw Data")
first_six <- substr(raw_file_names, 1, 6)
sub_ids <- unique(first_six)

length(sub_ids)

fast_RTs <- data.frame(ParticipantId = character(),
                       TrialName = character(),
                       type = character(),
                       time = numeric()
)

rt_data <- NULL

for (i in 1:length(sub_ids)){

  temp_main_file <- read_csv(paste0("Raw Data/", sub_ids[i], "_maindata.csv")) %>%
    mutate(AnswerRT = TimeToBeginInput - TimeToReadQuestion)
  
  #three potential RTs to exclude by: 
  ## total RT (reading + answering)
  ## reading RT
  ## answering RT (i'm thinking this one)
  
  if (any(temp_main_file$TimeToReadQuestion < 2000, na.rm = T)){
    which_index <- which(temp_main_file$TimeToReadQuestion < 2000)
    for (j in which_index){
      fast_RTs <- add_row(fast_RTs, ParticipantId = sub_ids[i],
                          TrialName = temp_main_file$TrialName[j],
                          type = "ReadingRT",
                          time = temp_main_file$TimeToReadQuestion[j])
    }
  }
  
  if (any(temp_main_file$AnswerRT < 2000, na.rm = T)){
    which_index <- which(temp_main_file$AnswerRT < 2000)
    for (j in which_index){
      fast_RTs <- add_row(fast_RTs, ParticipantId = sub_ids[i],
                          TrialName = temp_main_file$TrialName[j],
                          type = "AnswerRT",
                          time = temp_main_file$AnswerRT[j])
    }
  }
  
  if (any(temp_main_file$TimeToBeginInput < 2000, na.rm = T)){
    which_index <- which(temp_main_file$TimeToBeginInput < 2000)
    for (j in which_index) {
      fast_RTs <- add_row(fast_RTs, ParticipantId = sub_ids[i],
                          TrialName = temp_main_file$TrialName[j],
                          type = "TotalRT",
                          time = temp_main_file$TimeToBeginInput[j])
    }
  }
  
  rt_data <- rt_data %>%
    bind_rows(temp_main_file)
  
}

rt_data <- rt_data %>%
  rename(readRT = TimeToReadQuestion, totalRT = TimeToBeginInput)

#read in trialtype key (I created this from an early version of the previous paper)
trial_type_key <- read.csv("trial_type_key.csv", stringsAsFactors = F)

rt_data <- rt_data %>%
  mutate(TrialType = trial_type_key$TrialType[match(TrialName, trial_type_key$TrialName)]) %>%
  mutate(TrialType = paste0("Type",TrialType))


```

## Basic checks
```{r}
#does everyone have 17 trials

dim(rt_data)[1]
#122 participants, 17 trials
122*17

trials_per_participant <- rt_data %>% 
  group_by(ParticipantId, Condition) %>%
  summarize(n = n()) 

all(trials_per_participant$n == 17)

#how many participants per condition
subs_per_condition <- trials_per_participant %>%
  group_by(Condition) %>%
  summarize(nsubs = n())
#why is the balance so off?
kable(subs_per_condition)

```

## Remove outliers

```{r}

#removing on trial-by-trial basis

#remove answerRTs below 2000ms first
rt_data_remove <- rt_data %>%
  filter(AnswerRT >= 2000)
dim(rt_data_remove)[1]
#drops 7 trials

rt_data_summary <- rt_data %>%
  group_by(TrialName) %>%
  summarize(meanAnswerRT = mean(AnswerRT, na.rm = T),
            sdAnswerRT = sd(AnswerRT, na.rm = T),
            UB = meanAnswerRT + 3*sdAnswerRT,
            LB = meanAnswerRT - 3*sdAnswerRT)
rt_data_summary

rt_data_no_outliers <- rt_data_remove %>%
  group_by(TrialName) %>%
  filter((!(abs(AnswerRT - mean(AnswerRT)) > 3*sd(AnswerRT))))
dim(rt_data_no_outliers)[1]
#drops 47 more trials

rt_data_no_outliers %>%
  group_by(ParticipantId) %>%
  summarize(ntrials = n()) %>%
  with(hist(ntrials, breaks = 0:17))

##maybe consider replacing outliers with means instead of removing them?

```



## Compare conditions for question type (three types: identify, relate, predict)

```{r}
#compare read times (should be no differences of condition)
#compare answer times (potentially a difference) 


trial_type_means <- rt_data_no_outliers %>%
  group_by(ParticipantId, Condition, TrialType) %>%
  summarize(mean_readRT = mean(readRT),
            mean_answerRT = mean(AnswerRT),
            n = n())

# first, make .csv files in wide format to double check in statview
read_rt_type_wider <- trial_type_means %>%
  select(ParticipantId, Condition, TrialType, mean_readRT) %>%
  pivot_wider(names_from = TrialType,values_from = mean_readRT)
answer_rt_type_wider <- trial_type_means %>%
  select(ParticipantId, Condition, TrialType, mean_answerRT) %>%
  pivot_wider(names_from = TrialType,values_from = mean_answerRT)
write.csv(read_rt_type_wider, file = "readtypeRTs.csv", row.names = F)
write.csv(answer_rt_type_wider, file = "answertypeRTs.csv", row.names = F)


#### READ RTs ####

#make some plots

#just condition main effect
readplot1 <- rt_data_no_outliers %>% 
   group_by(ParticipantId, Condition) %>% 
   summarize(overallmean = mean(readRT)) %>% 
   group_by(Condition) %>% 
   summarize(overall_condition_mean = mean(overallmean), 
             se = std.error(overallmean), 
             n = n(), 
             CI = qt(0.975,df=n-1)*se) 

 ggplot(readplot1, aes(Condition, 
                       overall_condition_mean, 
                       group = 1, 
                       ymin = overall_condition_mean - CI, 
                       ymax = overall_condition_mean + CI)) + 
   theme_classic() +  
   geom_point(size = 4) + 
   geom_errorbar(width = .15, size = 0.85) +  
   geom_line(size = 0.85) + 
   labs(y = "Reading RTs (ms)", title = "Read RTs") 

#make a little plot using wide format
superbPlot(read_rt_type_wider,
    BSFactors   = "Condition", 
    WSFactors   = "QuestionType(3)", 
    variables   = c("Type1", "Type2", "Type3"),
    statistic   = "mean",
    errorbar    = "CI",
    gamma       = 0.95,
    adjustments = list(
        purpose       = "difference"
    ),
    plotStyle = "line",
    factorOrder = c("Condition", "QuestionType")
) + 
theme_classic() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))+
facet_wrap(vars(QuestionType))+
labs(y = "Reading RTs (ms)", title = "Read RTs")

read_rt_type_anova <- aov_ez(id = "ParticipantId",
                             dv = "mean_readRT",
                             data = trial_type_means,
                              within = "TrialType",
                              between = "Condition",
                             anova_table = list(es = "pes") #might want to double-check these
)

kable(nice(read_rt_type_anova))

#posthoc test for trial type
pairs(emmeans(read_rt_type_anova, "TrialType"), adjust = "Tukey")
#Question Type 2 slower than Type 1, marginally slower than Type 3 (for reading times)


#### ANSWER RTs ####

#make some plots

#just condition main effect
answerplot1 <- rt_data_no_outliers %>% 
   group_by(ParticipantId, Condition) %>% 
   summarize(overallmean = mean(AnswerRT)) %>% 
   group_by(Condition) %>% 
   summarize(overall_condition_mean = mean(overallmean), 
             se = std.error(overallmean), 
             n = n(), 
             CI = qt(0.975,df=n-1)*se) 

 ggplot(readplot1, aes(Condition, 
                       overall_condition_mean, 
                       group = 1, 
                       ymin = overall_condition_mean - CI, 
                       ymax = overall_condition_mean + CI)) + 
   theme_classic() +  
   geom_point(size = 4) + 
   geom_errorbar(width = .15, size = 0.85) +  
   geom_line(size = 0.85) + 
   labs(y = "Answering RTs (ms)", title = "Answer RTs") 
 
#make the little plot
superbPlot(answer_rt_type_wider,
    BSFactors   = "Condition", 
    WSFactors   = "QuestionType(3)", 
    variables   = c("Type1", "Type2", "Type3"),
    statistic   = "mean",
    errorbar    = "CI",
    gamma       = 0.95,
    adjustments = list(
        purpose       = "difference"
    ),
    plotStyle = "line",
    factorOrder = c("Condition", "QuestionType")
) + 
theme_classic() + 
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))+
facet_wrap(vars(QuestionType))+
labs(y = "Answering RTs (ms)", title = "Answer RTs")

answer_rt_type_anova <- aov_ez(id = "ParticipantId",
                             dv = "mean_answerRT",
                             data = trial_type_means,
                              within = "TrialType",
                              between = "Condition",
                             anova_table = list(es = "pes") #might want to double-check these
)

kable(nice(answer_rt_type_anova))

#posthoc test for trial type
pairs(emmeans(answer_rt_type_anova, "TrialType"), adjust = "Tukey")
#Question Type 3 much faster than Type 1/Type 2 (this is answering times)

ref <- emmeans(answer_rt_type_anova,~Condition|TrialType)

pairs(ref, adjust = "Tukey")

#plot and interaction suggests that the conditions have different effects for different
#question types. posthoc tests indicate a difference between VR and VRMonitor for QType 1




```

<!-- ## Compare conditions using individual questions -->

<!-- ```{r} -->

<!-- #compare read times (should be no differences of condition) -->
<!-- #compare answer times (potentially a difference) -->

<!-- # first, make .csv files in wide format to double check in statview -->
<!-- read_rt_wider <- rt_data_no_outliers %>% -->
<!--   select(ParticipantId, Condition, TrialName, readRT) %>% -->
<!--   pivot_wider(names_from = TrialName,values_from = readRT) %>% -->
<!--   mutate(noutliers = rowSums(is.na(across(3:19)))) -->

<!-- answer_rt_wider <- rt_data_no_outliers %>% -->
<!--   select(ParticipantId, Condition, TrialName, AnswerRT) %>% -->
<!--   pivot_wider(names_from = TrialName,values_from = AnswerRT) %>% -->
<!--   mutate(noutliers = rowSums(is.na(across(3:19))), -->
<!--          mean_all = rowMeans(across(3:19), na.rm = T)) -->

<!-- write.csv(read_rt_wider, file = "readRTs.csv", row.names = F) -->
<!-- write.csv(answer_rt_wider, file = "answerRTs.csv", row.names = F) -->


<!-- #within ANOVA can't run with missing cells, so remove participants that had outliers removed -->
<!-- #this loses a lot of data. would probably be better to replace outliers with means, but  -->
<!-- #i figure we don't really want to use individual question as the within variable anyway -->
<!-- #see below for analysis with question TYPE as the within variable -->
<!-- rt_data_no_outliers2 <- rt_data_no_outliers %>% -->
<!--   group_by(ParticipantId) %>% -->
<!--   filter(n() > 16) -->

<!-- #READING RTs -->

<!-- #make a little plot -->
<!-- readplot1 <- rt_data_no_outliers2 %>% -->
<!--   group_by(ParticipantId, Condition) %>% -->
<!--   summarize(overallmean = mean(readRT)) %>% -->
<!--   group_by(Condition) %>% -->
<!--   summarize(overall_condition_mean = mean(overallmean), -->
<!--             se = std.error(overallmean), -->
<!--             n = n(), -->
<!--             CI = qt(0.975,df=n-1)*se) -->

<!-- ggplot(readplot1, aes(Condition, -->
<!--                       overall_condition_mean, -->
<!--                       group = 1, -->
<!--                       ymin = overall_condition_mean - CI, -->
<!--                       ymax = overall_condition_mean + CI)) + -->
<!--   theme_classic() +  -->
<!--   geom_point(size = 4) + -->
<!--   geom_errorbar(width = .15, size = 0.85) +  -->
<!--   geom_line(size = 0.85) + -->
<!--   labs(y = "Reading RTs (ms)", title = "Read RTs") -->


<!-- read_rt_anova <- ezANOVA(rt_data_no_outliers2, -->
<!--                       dv = readRT, -->
<!--                       wid = ParticipantId, -->
<!--                       within = TrialName, -->
<!--                       between = Condition, -->
<!--                       type = 3, -->
<!--                       detailed = TRUE, -->
<!--                       return_aov = T -->
<!-- ) -->
<!-- #get the partial eta-squared too -->
<!-- aovEffectSize(read_rt_anova, effectSize = "pes")$ANOVA -->

<!-- #no significant effect of condition on read times -->

<!-- #ANSWERING RTs -->

<!-- #make a little plot -->
<!-- answerplot1 <- rt_data_no_outliers2 %>% -->
<!--   group_by(ParticipantId, Condition) %>% -->
<!--   summarize(overallmean = mean(AnswerRT)) %>% -->
<!--   group_by(Condition) %>% -->
<!--   summarize(overall_condition_mean = mean(overallmean), -->
<!--             se = std.error(overallmean), -->
<!--             n = n(), -->
<!--             CI = qt(0.975,df=n-1)*se) -->

<!-- ggplot(answerplot1, aes(Condition, -->
<!--                       overall_condition_mean, -->
<!--                       group = 1, -->
<!--                       ymin = overall_condition_mean - CI, -->
<!--                       ymax = overall_condition_mean + CI)) + -->
<!--   theme_classic() +  -->
<!--   geom_point(size = 4) + -->
<!--   geom_errorbar(width = .15, size = 0.85) +  -->
<!--   geom_line(size = 0.85) + -->
<!--   labs(y = "Answering RTs (ms)", title = "Answer RTs") -->

<!-- answer_rt_anova <- ezANOVA(rt_data_no_outliers2, -->
<!--                       dv = AnswerRT, -->
<!--                       wid = ParticipantId, -->
<!--                       within = TrialName, -->
<!--                       between = Condition, -->
<!--                       type = 3, -->
<!--                       detailed = TRUE, -->
<!--                       return_aov = T -->
<!-- ) -->
<!-- aovEffectSize(answer_rt_anova, effectSize = "pes")$ANOVA -->

<!-- #Post-hoc tests -->
<!-- #Remove Within Factor of TrialName (17 repeated question levels), causes problems with emmeans? -->
<!-- answer_rt_anova2 <- ezANOVA(rt_data_no_outliers2, -->
<!--                       dv = AnswerRT, -->
<!--                       wid = ParticipantId, -->
<!--                       #within = TrialName,  -->
<!--                       between = Condition, -->
<!--                       type = 3, -->
<!--                       detailed = F, -->
<!--                       return_aov = T -->
<!-- ) -->
<!-- emm.RT.answer <- emmeans(answer_rt_anova2$aov, ~ Condition) -->
<!-- emm.RT.answer -->
<!-- pairs(emm.RT.answer, adjust = "Tukey") -->
<!-- ``` -->

